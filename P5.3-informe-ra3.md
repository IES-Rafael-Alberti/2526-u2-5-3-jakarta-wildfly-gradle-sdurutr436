# a) Componentes y funcionamiento de los servicios del servidor

### Contenedor Docker:
Es el entorno aislado que ejecuta WildFly. Usa network namespaces para aislar su red del host y crea un puente virtual (docker0) que conecta el contenedor con la máquina host. Los puertos 8080 y 9990 están mapeados mediante NAT para permitir acceso externo.

### Wildfly Application Server:

Es el servidor de aplicaciones Jakarta EE que actúa como contenedor para tu aplicación. Internamente tiene:
- **Undertow**: El servidor web HTTP que escucha en el puerto 8080 y gestiona las peticiones entrantes.
- **Subsistemas**: CDI (inyección de dependencias), Jakarta REST (JAX-RS), JPA, transacciones, datasources, etc.
- **Deployment Scanner**: Monitoriza y despliega automáticamente los WAR.

### Archivo WAR:

Es el artefacto desplegable que contiene:
- Clases compiladas: la definición de la ruta base para REST, y el recurso que expone el CRUD. `Task.java` es el modelo de datos. El resto son archivos para que las tareas funcionen.
- Descriptores: `jboss-web.xml` define el contexto para personalizar la URL base, sin tener que modificar como se llama el archivo `.war`.
- Dependencias: Jakarta EE 10 API con Wildfly.

### Puertos:

**8080**: Puerto desde el que se escuchan las peticiones de la aplicación. Es el punto de entrada mapeado con Docker.

**9990**: Puerto donde se expone la consola de administración. Me permite configurar todo, monitorizar métricas y gestionar despliegues. También me permite modificar `standalone.xml` con advertencias por si hago algo mal.

### Endpoint REST (/api/tasks):

Es el rescurso Java de `TaskResource.java` cuyo `@Path("/tasks")` es el punto de partida. Todo se mapea a partir de aquí:
- `GET /api/tasks`: devuelve un JSON con todas las tareas.
- `GET /api/tasks/{id}`: devuelve la tarea con ese id o 404.
- `POST /api/tasks`: crea una nueva tarea. Necesario el titulo.
- `PUT /api/tasks/{id}`: actualiza una tarea existente.
- `DELETE /api/tasks/{id}`: elimina tarea por ID.

Las anotaciones que hay en el código indican que se trabaja con JSON.

---

## Flujo de usuario:

1. Cliente inicia la petición
2. Se procesa la petición y se manda al puerto 8080
3. Docker Desktop intercepta el tráfico, creando una red virtual. Wildfly tiene una IP asignada y se redirigen a ese contenedor.
4. Undertow recibe la conexión y parsea la petición HTTP.
5. Wildly busca el routing correcto por el contexto y la delega.
6. Jakarta REST analiza la ruta y procesa según la configuración.
7. Se crea una instancia de `TaskResource` si todo ha ido bien.
8. Se ejecuta la lógica de negocio, añadiendo la tarea a la lista (en caso de que sea añadir una por ejemplo).
9. Serialización de la respuesta: la `List<Task>` a JSON.
10. Construccion de la respuesta HTTP.
11. Docker Desktop reenvia la respuesta de forma inversa.
12. Cliente recibe y procesa mostrando el JSON.

---

## Evidencias parte a):

Contenedores y puertos publicados del contenedor Wildfly:

![Contenedor wildly](img/01-contenedor-docker-corriendo.png)

Despliegue y acceso del contenedor (sus logs):

![Logs de wildfly](img/04-logs-wildfly.png)

Respuesta del endpoit (previamente poblado porque esta vacio):

![Curl al endpoint de tasks](img/06-curl-3-tareas.png)

---

# b) Archivos principales de configuración y bibliotecas compartidas

La ruta del archivo de configuración:

```bash
docker exec -it wildfly ls -lh /opt/jboss/wildfly/standalone/configuration/
```

El comando me muestra:

![Archivos de configuracion Wildfly](img/09-archivo-de-configuracion-localizado.png)

Se podría tocar la configuración de **Datasources** que es donde configuro mi conexión a una base de datos, por ejemplo MySQL, para guardar las tareas en una base de datos en lugar de un JSON gigante.

También para cambiar los puertos e interfaces red para utilizar otros puertos y ajustar la ruta donde se guardan los logs.

---

La práctica pide que use ``build.gradle.kts`` de la práctica 5.2, pero en el repositorio del 5.3 no hay y hay un ``build.gradle`` que está teniendo prioridad sobre el ``.kts``. He recuperado y modificado el ``.kts`` y renombrado a ``old`` el otro para que se use el que pide la práctica.

```bash
dependencies {
    compileOnly("jakarta.platform:jakarta.jakartaee-api:10.0.0")

    testImplementation("org.junit.jupiter:junit-jupiter:5.10.2")
}
```

Evidencia:

![Salida completa](img/10-salida-nuevo-5.3.png)

# c) Cooperación con el servidor web (proxy / reverse proxy) y https

La arquitectura que propongo es:

Internet (a través del puerto 443 HTTPS) -> Nginx (mi contenedor alpine) -> Wildfly (puerto interno 8080, que no está expuesto).

Entonces en el entorno real de trabajo, lo que quedaría sería nginx que sería mi proxy reverso para recibir las peticiones de forma segura.

Esto cumple con el objetivo de publicar la API con rutas claras, sin exponer el puerto 9990 y cifrar el tráfico con HTTPS de forma externa, pero mantiene HTTP internamente entre los dos contenedores.

Como quedaría el nginx:

```nginx:

events {
    worker_connections 1024;
}

http {

    server {
        listen 80;
        server_name ejemplo.local;
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name ejemplo.local;

        ssl_certificate     /etc/nginx/ssl/server.crt;
        ssl_certificate_key /etc/nginx/ssl/server.key;
        ssl_protocols       TLSv1.2 TLSv1.3;

        # URL interna real: http://wildfly:8080/new-app/api/...
        # URL pública expuesta: https://ejemplo.local/api/...
        location /api/ {
            proxy_pass http://wildfly:8080/new-app/api/;
            proxy_set_header Host              $host;
            proxy_set_header X-Real-IP         $remote_addr;
            proxy_set_header X-Forwarded-For   $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto https;
        }

        location /console {
            return 403;
        }
    }
}
```

En esta configuración, Nginx escucha en los puertos 80 y 443, pero solo WildFly escucha en el 8080 de forma interna.

Que cambia entre la 5.2 y la 5.3 a este nuevo despliegue:

Actualmente:
- Se accede directamente a WildFly.
- El puerto 8080 está expuesto al host y si se mapea, puede acabar estando publicamente.
- El puerto 9990 también se encuentra expuesto si se publica mapeandolo.

Con la configuración de proxy:
- Los clientes externos solo ven a Nginx.
- El puerto 8080 de WildFly queda oculto en una red interna entre Nginx y WildFly.
- El puerto 9990 no se expone en Nginx, lo que hace que solo sea accesible desde red privada o máquina del adminsitrador.
- Las rutas son más limpias.

Y para la configuración de TLS y ventajas:

```bash
ssl_certificate     /etc/nginx/ssl/server.crt;
ssl_certificate_key /etc/nginx/ssl/server.key;
ssl_protocols       TLSv1.2 TLSv1.3;
```

Este fragmento activa el HTTPS en el frontal, usando un certificado SSL/TLS.

Las ventajas:
1. Cifrado de datos de tránsito: todo el tráfico va cifrado.
2. Autenticación del servidor: el navegador puede verificar que se está conectando al servidor.
3. Integridad de los datos: se asegura que en el tránsito los datos no se modifican.
4. Mejora experiencia y SEO: los navegadores marcan el lugar como "seguro".
5. Cumplimiento normativo: La legalidad lo pide.

¿Cambia algo en mi app?

No en mi app en si no, en WildFly, habría que cambiar en Undertow una aseguración de que el cliente original vino por HTTPS habilitando que haya cabeceras de forma correctas.

---

# d) Mecanismos de seguridad del servidor de aplicaciones

Lo que aplique en la 5.2:

1. Acceso a la consola de administración 9990 con usuario y contraseña.
2. Exposición de puertos limitada al entorno local exponiendo los puertos sólo en mi máquina local.
3. Gestión de secretos mínima, sin añadirlas en código fuente.
4. Logs activados por defecto, los cuales me han permitido ver el despliegue de WAR, los accesos a endpoints y mensajes de mi `TaskResource`.

## Medidas adicionales para un entorno de producción:

1. Consola de administración y sus credenciales.

Medida 1:
- En lugar de `-p 9990:9990`, directamente no publico ese puerto en Docker.
- El acceso solo es desde VPN o red interna.

Al no exponerla, evito ataques de fuerza bruta.

Medida 2:
- Los usuarios administrativos deben tener contraseñas únicas y robustas.
- Aplicar control de acceso basado en roles para que no todos los administradores tengan los mismos permisos.

Esto permite que si una cuenta falla, no haga un daño gigante.

2. Exposición de puertos:

Medida 3:
- Colocar nginx (que ahora mismo no hay) frente a WildFly para exponer los puertos que yo quiera.

3. Gestión de secretos:

Medida 4:
- Usar variables de entorno o mecanismos de secretos en lugar de usuarios/contraseñas en DockerFiles y tal.

Medida 5:
- Separar los usuarios de administración de los de aplicación, usando los que ya tiene WildFly por defecto, pero volviendo a la parte 2, separarlos por roles incluso dentro de los de administración.

4. Registro y trazabilidad.

Medida 6:
- Configurar el subsistema de loggin para enviar logs a un fichero dedicado, separar logs de aplicación de los de servidor y logs de acceso HTTP en UnderTow.

Estas serían algunas de las medidas de seguridad que llevaría a cabo para que mi entorno de producción fuese totalmente seguro.

Evidencias de la configuración actual.

Puertos expuestos:
![Puertos expuestos](img/11-puertos-expuestos.png)

Consola de administración 9990:
![Consola administración](img/12-consola-administracion-9990.png)

Política actual de secretos:
![Secretos guardados en el contenedor](img/13-politica-de-secretos.png)

En producción, las credenciales de base de datos y otros secretos se inyectarían como variables de entorno o mediante un sistema de secretos (no en el código fuente ni en el repositorio), aunque ahora mismo los hashes de contraseña se guardan en el contenedor directamente.

---

# e) Componentes web del servidor de aplicaciones

## Qué es el WAR y que contiene:

La aplicación REST se empaqueta en un WAR, es un zip especial que conteine todo lo necesario para desplegar una aplicación web Jakarta EE: clases java, recursos estáticos...

En la 5.3 lo que tiene son clases con rutas para peiticiones de API, y los modelos y su persistencia, además de un WEB-INF, que gracias a jboss, enmascaro la ruta con un nombre presentable en lugr de directamente el nombre del `.war`.

Todo esto forma parte de la aplicación que WildFly despliega.

## Qué significa el contexto / ruta base de la app.

Cuando se despliega el WAR, se le asigna un **context root** de despliegue. Esto se define gracias al ``jboss-web.xml``.

Esto significa que todo lo que hay dentro de ese WAR se sirve bajo la URL base. A partir de ahí se le añaden las rutas definidas en las @ApplicationPath y @Path de Java.

## Qué parte del servidor sirve la API:

En WildFly, el componente que se encarga de recibir y servir las peticiones es el subsistema Undertow, que actua como contenedor web.

Undertow escucha en el puerto 8080 (HTTP) y 8443 (HTTPS interno). Cuando llega una petición HTTP:

- Mira la URL.
- Decide qué contexto (/new-app, /myproject/module/backend, etc.) debe manejarla.
- Dentro de ese contexto, delega a la implementación de Jakarta REST (RESTEasy) para mapear la URL a tus recursos.

## Desglose de URL real:

``http://localhost:8080/new-app/api/tasks``

- http → Protocolo (sin cifrado en este despliegue de prácticas).
- localhost → Host (tu propia máquina).
- 8080 → Puerto donde Undertow (WildFly) escucha peticiones HTTP.
- /new-app → Context root del WAR crud-file-1.0.0.war, definido en jboss-web.xml.
- /api → Ruta base REST, definida por @ApplicationPath("/api") en TasksApplication.
- /tasks → Recurso REST concreto, definido por @Path("/tasks") en TaskResource.

Evidencias:

![Curl hacia endpoint real](img/14-curl-end-point-real.png)

![Logs que confirman el lanzamiento correcto](img/15-logs-confirmando-hora.png)

